{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ct6xY1DP3VNt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X9FbV_O94JEz"
   },
   "outputs": [],
   "source": [
    "# initialize for  each parameters\n",
    "DATASET = 'FashionMNIST'\n",
    "BATCH_SIZE = 100\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "WEIGHT_DECAY = 0.007\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "SCHEDULER_STEPS = 100\n",
    "SCHEDULER_GAMMA = 0.1\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "EPOCH = 150\n",
    "\n",
    "KD_LAMBDA = 2.0\n",
    "\n",
    "TRIPLET_MARGINE = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd_scratch/cvit/sashank.sridhar/FashionMnist-KD-HKD\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "veXjheeA4LhC"
   },
   "outputs": [],
   "source": [
    "# fixing the seed\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnVR6kht4NZh",
    "outputId": "1a5b33ee-df16-440e-d107-ee70483ce6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu mode\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"gpu mode\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"cpu mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nS54WHBh4Pdg"
   },
   "outputs": [],
   "source": [
    "# the name of results files\n",
    "codename = 'fashion_mnist_kd_hkd_rd'\n",
    "\n",
    "fnnname = codename + \"_fnn_model\"\n",
    "\n",
    "total_loss_name = codename + \"_total_loss\"\n",
    "soft_loss_name = codename + \"_soft_loss\"\n",
    "tri_loss_name = codename + \"_tri_loss\"\n",
    "acc_name = codename + \"_accuracy\"\n",
    "\n",
    "result_name = codename + \"_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-uCnZzVc4S-k"
   },
   "outputs": [],
   "source": [
    "class Datasets(object):\n",
    "    def __init__(self, dataset_name, batch_size = 100, num_workers = 2, transform = None, shuffle = True):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def create(self, path = None):\n",
    "        print(\"Dataset :\",self.dataset_name)\n",
    "        if self.transform is None:\n",
    "                self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "        \n",
    "        if path is None:\n",
    "            path = \"./\"+self.dataset_name+\"Dataset/data\"\n",
    "        \n",
    "        \n",
    "        if self.dataset_name == \"MNIST\":\n",
    "            trainset = torchvision.datasets.MNIST(root = path,\n",
    "                                       train = True, download = True, transform = self.transform)\n",
    "            testset = torchvision.datasets.MNIST(root = path,\n",
    "                                                 train = False, download = True, transform = self.transform)\n",
    "            classes = list(range(10))\n",
    "            base_labels = trainset.classes\n",
    "            \n",
    "        elif self.dataset_name == \"FashionMNIST\":\n",
    "            trainset = torchvision.datasets.FashionMNIST(root = path,\n",
    "                                       train = True, download = True, transform = self.transform)\n",
    "            testset = torchvision.datasets.FashionMNIST(root = path,\n",
    "                                                 train = False, download = True, transform = self.transform)\n",
    "            classes = list(range(10))\n",
    "            base_labels = trainset.classes\n",
    "            \n",
    "        elif self.dataset_name == \"CIFAR10\":\n",
    "            trainset = torchvision.datasets.CIFAR10(root = path,\n",
    "                                       train = True, download = True, transform = self.transform)\n",
    "            testset = torchvision.datasets.CIFAR10(root = path,\n",
    "                                                 train = False, download = True, transform = self.transform)\n",
    "            classes = list(range(10))\n",
    "            base_labels = trainset.classes\n",
    "            \n",
    "        elif self.dataset_name == \"CIFAR100\":\n",
    "            trainset = torchvision.datasets.CIFAR100(root = path,\n",
    "                                       train = True, download = True, transform = self.transform)\n",
    "            testset = torchvision.datasets.CIFAR100(root = path,\n",
    "                                                 train = False, download = True, transform = self.transform)\n",
    "            classes = list(range(100))\n",
    "            base_labels = trainset.classes\n",
    "        \n",
    "        else:\n",
    "            raise KeyError(\"Unknown dataset: {}\".format(self.dataset_name))\n",
    "            \n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size = self.batch_size,\n",
    "                        shuffle = self.shuffle, num_workers = self.num_workers)\n",
    "        \n",
    "        if testset is not None:\n",
    "            testloader = torch.utils.data.DataLoader(testset, batch_size = self.batch_size,\n",
    "                        shuffle = False, num_workers = self.num_workers)\n",
    "        else:\n",
    "            testloader = None\n",
    "            \n",
    "            \n",
    "        return [trainloader, testloader, classes, base_labels, trainset, testset]\n",
    "    \n",
    "    def worker_init_fn(self, worker_id):                                                          \n",
    "        np.random.seed(worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "1684bb7895874598b0cb3c8344ce4851",
      "185cff25aff34094a39c57dbf63e1975",
      "cd02fad8000c476ebea64496837d52c3",
      "4e65525bb7fa4638964b72c3490847fb",
      "cd38ab54d77d4b0eb9f027e96800096a",
      "f77ef129c307426b9aaf83dd61dcc5d5",
      "7a07a97d997744fab665c96c1f5fe579",
      "9c23bae705e54a6084b3fd4282a4ecdc",
      "6c49593980e64b7fac5d3ce14efb325c",
      "85f298067f814dd19f731aebea5d6e96",
      "b372328dca9b49d4960f05df1a50d60a"
     ]
    },
    "id": "qLqmqtjc4ZzB",
    "outputId": "fab5611e-6dc2-4f95-b78f-cbb348fd6daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : FashionMNIST\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FashionMNISTDataset/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba34c8753af04e4893e1773c1ff435a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FashionMNISTDataset/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNISTDataset/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FashionMNISTDataset/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97ae6baef1546ff8a44a6da782e2570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FashionMNISTDataset/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNISTDataset/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNISTDataset/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c857fe6f0a4a4af9916d48b3f52b10f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FashionMNISTDataset/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNISTDataset/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNISTDataset/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9164f87219ca4712aa3de1ea60031573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FashionMNISTDataset/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNISTDataset/data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data set\n",
    "instance_datasets = Datasets(DATASET, BATCH_SIZE, NUM_WORKERS, shuffle = False)\n",
    "data_sets = instance_datasets.create()\n",
    "\n",
    "#trainloader = data_sets[0]\n",
    "#testloader = data_sets[1]\n",
    "classes = data_sets[2]\n",
    "based_labels = data_sets[3]\n",
    "trainset = data_sets[4]\n",
    "testset = data_sets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E6IHN1HX4e-K"
   },
   "outputs": [],
   "source": [
    "class KDTripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        data = dataset.data\n",
    "        labels = dataset.targets\n",
    "        if type(labels) is not torch.Tensor:\n",
    "                labels = torch.tensor(labels)\n",
    "        \n",
    "        # make label set 0-9\n",
    "        labels_set = set(labels.numpy())\n",
    "        \n",
    "        # make the indices excepted each classes\n",
    "        label_to_indices = {label : np.where(labels.numpy() != label)[0] for label in labels_set}\n",
    "        \n",
    "        if self.dataset.train:\n",
    "            self.negative_indices = label_to_indices\n",
    "        else:\n",
    "            self.negative_indices = [[np.random.choice(label_to_indices[labels[i].item()])] for i in range(len(data))]\n",
    "        \n",
    "\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        if self.dataset.train:\n",
    "            img1_2, label1_2 = self.dataset[index]\n",
    "            if type(label1_2) is not torch.Tensor:\n",
    "                label1_2 = torch.tensor(label1_2)\n",
    "            img3, label3 = self.dataset[np.random.choice(self.negative_indices[label1_2.item()])]\n",
    "        else:\n",
    "            img1_2, label1_2 = self.dataset[index]\n",
    "            img3, label3 = self.dataset[self.negative_indices[index][0]]\n",
    "        \n",
    "            \n",
    "        return (img1_2, img3), (label1_2, label3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9GMBjWBp4lWX"
   },
   "outputs": [],
   "source": [
    "# use the KD Triplet Dataset by using above dataset\n",
    "tri_trainset = KDTripletDataset(trainset)\n",
    "tri_testset = KDTripletDataset(testset)\n",
    "tri_trainloader = torch.utils.data.DataLoader(tri_trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
    "tri_testloader = torch.utils.data.DataLoader(tri_testset, batch_size = BATCH_SIZE, shuffle = False, num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eN-5D93D4t6r"
   },
   "outputs": [],
   "source": [
    "class TeacherNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "        self.dropout_input = 0.0\n",
    "        self.dropout_hidden = 0.0\n",
    "        self.is_training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.dropout(x, p=self.dropout_input, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class StudentNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 400)\n",
    "        self.fc2 = nn.Linear(400, 10)\n",
    "        self.dropout_input = 0.0\n",
    "        self.dropout_hidden = 0.0\n",
    "        self.is_training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.dropout(x, p=self.dropout_input, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bat_size = 100 \n",
      "loss = CrossEntropyLoss()\n",
      "optimizer = SGD\n",
      "max_epochs =  20 \n",
      "lrn_rate = 0.005 \n",
      "\n",
      "Starting training\n",
      "Iteration: 600/600train mean loss=214.56449995438257, accuracy=0.5914333333333328\n",
      "Iteration: 600/600train mean loss=138.40460643172264, accuracy=0.768183333333333\n",
      "Iteration: 600/600train mean loss=74.1871007680893, accuracy=0.8315833333333333\n",
      "Iteration: 600/600train mean loss=54.11694197356701, accuracy=0.8622833333333335\n",
      "Iteration: 600/600train mean loss=45.53262098878622, accuracy=0.8792833333333342\n",
      "Iteration: 600/600train mean loss=40.77618765706817, accuracy=0.8889666666666668\n",
      "Iteration: 600/600train mean loss=37.73662828281522, accuracy=0.8952500000000005\n",
      "Iteration: 600/600train mean loss=35.58648294645051, accuracy=0.9002000000000002\n",
      "Iteration: 600/600train mean loss=33.94134741773208, accuracy=0.9046833333333334\n",
      "Iteration: 600/600train mean loss=32.60682564539214, accuracy=0.9082500000000007\n",
      "Iteration: 600/600train mean loss=31.476194728165865, accuracy=0.910900000000001\n",
      "Iteration: 600/600train mean loss=30.487235512584448, accuracy=0.9133166666666672\n",
      "Iteration: 600/600train mean loss=29.6012119303147, accuracy=0.915933333333334\n",
      "Iteration: 600/600train mean loss=28.793700205162168, accuracy=0.9184333333333345\n",
      "Iteration: 600/600train mean loss=28.047793610021472, accuracy=0.9203166666666673\n",
      "Iteration: 600/600train mean loss=27.350947209323447, accuracy=0.922700000000001\n",
      "Iteration: 600/600train mean loss=26.693703729038436, accuracy=0.9245500000000018\n",
      "Iteration: 600/600train mean loss=26.069298963372905, accuracy=0.9260666666666678\n",
      "Iteration: 600/600train mean loss=25.472886502432328, accuracy=0.9277666666666683\n",
      "Iteration: 600/600train mean loss=24.900267854022484, accuracy=0.9292500000000022\n",
      "Done \n"
     ]
    }
   ],
   "source": [
    "net = TeacherNetwork()\n",
    "bat_size = 100\n",
    "max_epochs = 20  # 100 gives better results\n",
    "ep_log_interval = 5\n",
    "lrn_rate = 0.005\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()  # does log-softmax()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lrn_rate)\n",
    "\n",
    "print(\"\\nbat_size = %3d \" % bat_size)\n",
    "print(\"loss = \" + str(loss_func))\n",
    "print(\"optimizer = SGD\")\n",
    "print(\"max_epochs = %3d \" % max_epochs)\n",
    "print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
    "\n",
    "print(\"\\nStarting training\")\n",
    "net.train()  # set mode\n",
    "\n",
    "for epoch in range(0, max_epochs):\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    ep_loss = 0  # for one full epoch\n",
    "    c = 0\n",
    "    for (batch_idx, batch) in enumerate(data_sets[0]):\n",
    "        c+=1\n",
    "        print(\"\\rIteration: {}/{}\".format(c, len(data_sets[0])), end=\"\")\n",
    "        (X, y) = batch  # X = pixels, y = target labels\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        X.to(device)\n",
    "#         print(X.shape)\n",
    "        y.to(device)\n",
    "        oupt = net(X)\n",
    "        loss_val = loss_func(oupt, y)  # a tensor\n",
    "        ep_loss += loss_val.item()  # accumulate\n",
    "        loss_val.backward()  # compute grads\n",
    "        optimizer.step()     # update weights\n",
    "        accuracy += float(torch.sum(torch.argmax(oupt, dim=1) == y).item()) / y.shape[0]\n",
    "    print(\"train mean loss={}, accuracy={}\".format(ep_loss*bat_size/len(data_sets[0]), accuracy/len(data_sets[0])))\n",
    "print(\"Done \") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"Mnist_teacher.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2Vay8DfU4y3z"
   },
   "outputs": [],
   "source": [
    "# network and criterions\n",
    "model_t = TeacherNetwork().to(device)\n",
    "model_s = StudentNetwork().to(device)\n",
    "\n",
    "model_t.load_state_dict(torch.load(\"FashionMnist_teacher.pt\"))\n",
    "\n",
    "optimizer = optim.SGD(model_s.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEPS, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "soft_criterion = nn.CrossEntropyLoss()\n",
    "triplet_loss = nn.TripletMarginLoss(margin=TRIPLET_MARGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "18uppIPYXFG2"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(e, squared=False, eps=1e-12):\n",
    "    e_square = e.pow(2).sum(dim=1)\n",
    "    prod = e @ e.t()\n",
    "    res = (e_square.unsqueeze(1) + e_square.unsqueeze(0) - 2 * prod).clamp(min=eps)\n",
    "\n",
    "    if not squared:\n",
    "        res = res.sqrt()\n",
    "\n",
    "    res = res.clone()\n",
    "    res[range(len(e)), range(len(e))] = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RkdDistance(nn.Module):\n",
    "    def forward(self, student, teacher):\n",
    "        with torch.no_grad():\n",
    "            t_d = pdist(teacher, squared=False)\n",
    "            mean_td = t_d[t_d>0].mean()\n",
    "            t_d = t_d / mean_td\n",
    "\n",
    "        d = pdist(student, squared=False)\n",
    "        mean_d = d[d>0].mean()\n",
    "        d = d / mean_d\n",
    "\n",
    "        loss = F.smooth_l1_loss(d, t_d, reduction='elementwise_mean')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RKdAngle(nn.Module):\n",
    "    def forward(self, student, teacher):\n",
    "        # N x C\n",
    "        # N x N x C\n",
    "\n",
    "        with torch.no_grad():\n",
    "            td = (teacher.unsqueeze(0) - teacher.unsqueeze(1))\n",
    "            norm_td = F.normalize(td, p=2, dim=2)\n",
    "            t_angle = torch.bmm(norm_td, norm_td.transpose(1, 2)).view(-1)\n",
    "\n",
    "        sd = (student.unsqueeze(0) - student.unsqueeze(1))\n",
    "        norm_sd = F.normalize(sd, p=2, dim=2)\n",
    "        s_angle = torch.bmm(norm_sd, norm_sd.transpose(1, 2)).view(-1)\n",
    "\n",
    "        loss = F.smooth_l1_loss(s_angle, t_angle, reduction='elementwise_mean')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardDarkRank(nn.Module):\n",
    "    def __init__(self, alpha=3, beta=3, permute_len=4):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.permute_len = permute_len\n",
    "\n",
    "    def forward(self, student, teacher):\n",
    "        score_teacher = -1 * self.alpha * pdist(teacher, squared=False).pow(self.beta)\n",
    "        score_student = -1 * self.alpha * pdist(student, squared=False).pow(self.beta)\n",
    "\n",
    "        permute_idx = score_teacher.sort(dim=1, descending=True)[1][:, 1:(self.permute_len+1)]\n",
    "        ordered_student = torch.gather(score_student, 1, permute_idx)\n",
    "\n",
    "        log_prob = (ordered_student - torch.stack([torch.logsumexp(ordered_student[:, i:], dim=1) for i in range(permute_idx.size(1))], dim=1)).sum(dim=1)\n",
    "        loss = (-1 * log_prob).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "MZpk2uTX44GD"
   },
   "outputs": [],
   "source": [
    "class NetworkFit(object):\n",
    "    def __init__(self, model_t, model_s, optimizer, soft_criterion, distance_criterion,angle_criterion, dark_criterion, triplet_loss):\n",
    "        self.model_t = model_t\n",
    "        self.model_s = model_s\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.soft_criterion = soft_criterion\n",
    "        self.triplet_loss = triplet_loss\n",
    "        \n",
    "        self.distance_criterion = distance_criterion\n",
    "        \n",
    "        self.angle_criterion = angle_criterion\n",
    "        \n",
    "        self.dark_criterion = dark_criterion\n",
    "        \n",
    "        self.model_t.eval()\n",
    "        \n",
    "\n",
    "    def train(self, inputs, labels, kd_lambda = 2.0):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model_s.train()\n",
    "\n",
    "        img1_t = inputs[0]\n",
    "        img2_s = inputs[1]\n",
    "        img3_s = inputs[2]\n",
    "        \n",
    "        label1_t = labels[0]\n",
    "        label2_s = labels[1]\n",
    "        label3_s = labels[2]\n",
    "        \n",
    "        out1_t = self.model_t(img1_t)\n",
    "        out2_s = self.model_s(img2_s)\n",
    "        out3_s = self.model_s(img3_s)\n",
    "        \n",
    "        soft_loss = self.soft_criterion(out2_s, label2_s)\n",
    "        trip_loss = self.triplet_loss(out1_t, out2_s, out3_s)\n",
    "\n",
    "        temperature=1.0\n",
    "        \n",
    "        soft_log_probs = F.log_softmax(out2_s / temperature, dim=1)\n",
    "        # soft_targets = F.softmax(self.cached_teacher_logits[minibatch_id] / self.temperature)\n",
    "        soft_targets = F.softmax(out1_t / temperature, dim=1)\n",
    "\n",
    "        distillation_loss = F.kl_div(soft_log_probs, soft_targets.detach(), reduction='batchmean')\n",
    "        \n",
    "        distillation_loss_scaled = distillation_loss * temperature ** 2\n",
    "        \n",
    "        dist_ratio = 1.0\n",
    "        \n",
    "        dist_loss = dist_ratio * self.distance_criterion(out2_s, out1_t)\n",
    "        \n",
    "        angle_ratio = 1.0\n",
    "        \n",
    "        angle_loss = angle_ratio * self.angle_criterion(out2_s, out1_t)\n",
    "        \n",
    "        dark_ratio = 1.0\n",
    "        \n",
    "        dark_loss = dark_ratio * self.dark_criterion(out2_s, out1_t)\n",
    "\n",
    "        loss = soft_loss + kd_lambda*trip_loss + distillation_loss_scaled + dist_loss + angle_loss + dark_loss\n",
    " \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "            \n",
    "    def test(self, inputs, labels, kd_lambda = 2.0):\n",
    "        self.model_s.eval()\n",
    "        \n",
    "        img1_t = inputs[0]\n",
    "        img2_s = inputs[1]\n",
    "        img3_s = inputs[2]\n",
    "        \n",
    "        label1_t = labels[0]\n",
    "        label2_s = labels[1]\n",
    "        label3_s = labels[2]\n",
    "        \n",
    "        out1_t = self.model_t(img1_t)\n",
    "        out2_s = self.model_s(img2_s)\n",
    "        out3_s = self.model_s(img3_s)\n",
    "        \n",
    "        soft_loss = self.soft_criterion(out2_s, label2_s)\n",
    "        trip_loss = self.triplet_loss(out1_t, out2_s, out3_s)\n",
    "        \n",
    "        temperature=1.0\n",
    "        \n",
    "        soft_log_probs = F.log_softmax(out2_s / temperature, dim=1)\n",
    "        # soft_targets = F.softmax(self.cached_teacher_logits[minibatch_id] / self.temperature)\n",
    "        soft_targets = F.softmax(out1_t / temperature, dim=1)\n",
    "\n",
    "        distillation_loss = F.kl_div(soft_log_probs, soft_targets.detach(), reduction='batchmean')\n",
    "        \n",
    "        distillation_loss_scaled = distillation_loss * temperature ** 2\n",
    "\n",
    "        dist_ratio = 1.0\n",
    "        \n",
    "        dist_loss = dist_ratio * self.distance_criterion(out2_s, out1_t)\n",
    "        \n",
    "        angle_ratio = 1.0\n",
    "        \n",
    "        angle_loss = angle_ratio * self.angle_criterion(out2_s, out1_t)\n",
    "        \n",
    "        dark_ratio = 1.0\n",
    "        \n",
    "        dark_loss = dark_ratio * self.dark_criterion(out2_s, out1_t)\n",
    "\n",
    "        loss = soft_loss + kd_lambda*trip_loss + distillation_loss_scaled + dist_loss + angle_loss + dark_loss\n",
    "        \n",
    "        _, predicted = out2_s.max(1)\n",
    "        correct = (predicted == label2_s).sum().item()\n",
    "        \n",
    "        return [loss.item(), soft_loss.item(), trip_loss.item()], [correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AHRH5T6J5VP-"
   },
   "outputs": [],
   "source": [
    "# fit for training and test\n",
    "dist_criterion = RkdDistance()\n",
    "angle_criterion = RKdAngle()\n",
    "dark_criterion = HardDarkRank()\n",
    "\n",
    "fit = NetworkFit(model_t, model_s, optimizer, soft_criterion,dist_criterion,angle_criterion,dark_criterion, triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "j9BF3qbE5W--"
   },
   "outputs": [],
   "source": [
    "class Score(object):\n",
    "    def __init__(self, score = 0):\n",
    "        self.score = score\n",
    "        \n",
    "    def sum_score(self, score):\n",
    "        self.score += score\n",
    "    \n",
    "    def set_score(self, score):\n",
    "        self.score = score\n",
    "    \n",
    "    def init_score(self):\n",
    "        self.score = 0\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "U0gzfZ-n5hA0"
   },
   "outputs": [],
   "source": [
    "class ScoreCalc(object):\n",
    "    def __init__(self, losses, corrects, batch_size):\n",
    "        self.losses = losses\n",
    "        self.corrects = corrects\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.len_l = len(losses)\n",
    "        self.len_c = len(corrects)\n",
    "        \n",
    "        self.train_losses = [[] for l in range(self.len_l)]\n",
    "        self.train_corrects = [[] for c in range(self.len_c)]\n",
    "        \n",
    "        self.test_losses = [[] for l in range(self.len_l)]\n",
    "        self.test_corrects = [[] for c in range(self.len_c)]\n",
    "\n",
    "        patience = 10\n",
    "\n",
    "        self.early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "       \n",
    "    \n",
    "    def calc_sum(self, losses, corrects):\n",
    "        if len(losses) != len(self.losses):\n",
    "            print(\"warning : len(losses) != len(self.losses)\")\n",
    "            sys.exit()\n",
    "        if len(corrects) != len(self.corrects):\n",
    "            print(\"warning : len(corrects) != len(self.corrects)\")\n",
    "            sys.exit()\n",
    "        \n",
    "        for l in range(self.len_l):\n",
    "            self.losses[l].sum_score(losses[l])\n",
    "        \n",
    "        for c in range(self.len_c):\n",
    "            self.corrects[c].sum_score(corrects[c])\n",
    "        \n",
    "        return self.losses, self.corrects\n",
    "    \n",
    "    \n",
    "    def score_del(self):\n",
    "        for loss in self.losses:\n",
    "            loss.init_score()\n",
    "        for correct in self.corrects:\n",
    "            correct.init_score()\n",
    "\n",
    "        \n",
    "    def score_print(self, data_num, train = True):\n",
    "        if train:\n",
    "            print(\"train mean loss={}, accuracy={}\".format(self.losses[0].get_score()*self.batch_size/data_num, float(self.corrects[0].get_score()/data_num)))\n",
    "        else:\n",
    "            \n",
    "            print(\"test mean loss={}, accuracy={}\".format(self.losses[0].get_score()*self.batch_size/data_num, float(self.corrects[0].get_score()/data_num)))\n",
    "            self.early_stopping(self.losses[0].get_score()*self.batch_size/data_num, model_s)\n",
    "        \n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                return True\n",
    "            else:\n",
    "              return False\n",
    "\n",
    "            \n",
    "    def score_append(self, data_num, train = True):\n",
    "        if train:\n",
    "            for l in range(self.len_l):\n",
    "                self.train_losses[l].append(self.losses[l].get_score()*self.batch_size/data_num)\n",
    "            for c in range(self.len_c):\n",
    "                self.train_corrects[c].append(float(self.corrects[c].get_score()/data_num))\n",
    "        else:\n",
    "            for l in range(self.len_l):\n",
    "                self.test_losses[l].append(self.losses[l].get_score()*self.batch_size/data_num)\n",
    "            for c in range(self.len_c):\n",
    "                self.test_corrects[c].append(float(self.corrects[c].get_score()/data_num))\n",
    "    \n",
    "    \n",
    "    def get_value(self, train = True):\n",
    "        if train:\n",
    "            return self.train_losses, self.train_corrects\n",
    "        else:\n",
    "            return self.test_losses, self.test_corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "W4ogtBkH5cHu"
   },
   "outputs": [],
   "source": [
    "# to manage all scores\n",
    "loss = Score()\n",
    "loss_s = Score()\n",
    "loss_t = Score()\n",
    "correct = Score()\n",
    "score_loss = [loss, loss_s, loss_t]\n",
    "score_correct = [correct]\n",
    "sc = ScoreCalc(score_loss, score_correct, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxeumM7g5czi",
    "outputId": "ce7864d0-f67e-4fe8-dd88-9b9aa803ab43",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Iteration: 1/600"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sashank.sridhar/miniconda3/envs/TripletLoss/lib/python3.9/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.70162922859192, accuracy=0.55365\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.720994510650634, accuracy=0.5472\n",
      "Validation loss decreased (inf --> 10.720995).  Saving model ...\n",
      "epoch 2\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.49347453435262, accuracy=0.57595\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.393991327285766, accuracy=0.576\n",
      "Validation loss decreased (10.720995 --> 10.393991).  Saving model ...\n",
      "epoch 3\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.277809816996257, accuracy=0.6196333333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.30991798400879, accuracy=0.6238\n",
      "Validation loss decreased (10.393991 --> 10.309918).  Saving model ...\n",
      "epoch 4\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.160593059062958, accuracy=0.62585\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.123002099990845, accuracy=0.622\n",
      "Validation loss decreased (10.309918 --> 10.123002).  Saving model ...\n",
      "epoch 5\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=9.695070892969767, accuracy=0.6591333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=9.711372165679931, accuracy=0.6531\n",
      "Validation loss decreased (10.123002 --> 9.711372).  Saving model ...\n",
      "epoch 6\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.257525137265523, accuracy=0.5308\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.24606722831726, accuracy=0.5261\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch 7\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.67409787972768, accuracy=0.5967333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.6995175075531, accuracy=0.59\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch 8\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=11.411739277839661, accuracy=0.5348166666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=11.383002119064331, accuracy=0.5351\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch 9\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.472427457173666, accuracy=0.5967\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.509432125091553, accuracy=0.589\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch 10\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.291583644549052, accuracy=0.5827333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.314282894134521, accuracy=0.5807\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch 11\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=11.898341013590494, accuracy=0.6067833333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=11.878096380233764, accuracy=0.6003\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch 12\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.882992003758748, accuracy=0.5678\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.89766474723816, accuracy=0.562\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch 13\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=9.991538988749186, accuracy=0.5391333333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.026660280227661, accuracy=0.5399\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch 14\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.381221402486165, accuracy=0.5349166666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.373179626464843, accuracy=0.5315\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch 15\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=10.03362847963969, accuracy=0.5703666666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=10.056722688674927, accuracy=0.571\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# training and test\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch', epoch+1)\n",
    "    \n",
    "    c = 0\n",
    "    for (inputs, labels) in tri_trainloader:\n",
    "        c+=1\n",
    "        print(\"\\rIteration: {}/{}\".format(c, len(tri_trainloader)), end=\"\")\n",
    "        img1_t = inputs[0].to(device)\n",
    "        img2_s = inputs[0].to(device)\n",
    "        img3_s = inputs[1].to(device)\n",
    "        \n",
    "        images = (img1_t, img2_s, img3_s)\n",
    "        \n",
    "        label1_t = labels[0].to(device)\n",
    "        label2_s = labels[0].to(device)\n",
    "        label3_s = labels[1].to(device)\n",
    "        \n",
    "        label = (label1_t, label2_s, label3_s)\n",
    "        \n",
    "        fit.train(images, label, KD_LAMBDA)\n",
    "    c = 0\n",
    "    print(\"Train Loss Calc\")\n",
    "    for (inputs, labels) in tri_trainloader:\n",
    "        c+=1\n",
    "        print(\"\\rIteration: {}/{}\".format(c, len(tri_trainloader)), end=\"\")\n",
    "        img1_t = inputs[0].to(device)\n",
    "        img2_s = inputs[0].to(device)\n",
    "        img3_s = inputs[1].to(device)\n",
    "        \n",
    "        images = (img1_t, img2_s, img3_s)\n",
    "        \n",
    "        label1_t = labels[0].to(device)\n",
    "        label2_s = labels[0].to(device)\n",
    "        label3_s = labels[1].to(device)\n",
    "        \n",
    "        label = (label1_t, label2_s, label3_s)\n",
    "        \n",
    "        losses, corrects = fit.test(images, label, KD_LAMBDA)\n",
    "        \n",
    "        sc.calc_sum(losses, corrects)\n",
    "    \n",
    "    sc.score_print(len(trainset))\n",
    "    sc.score_append(len(trainset))\n",
    "    sc.score_del()\n",
    "    c = 0\n",
    "    print(\"Test Loss Calc\")\n",
    "    for (inputs, labels) in tri_testloader:\n",
    "        c+=1\n",
    "        \n",
    "        print(\"\\rIteration: {}/{}\".format(c, len(tri_testloader)), end=\"\")\n",
    "        img1_t = inputs[0].to(device)\n",
    "        img2_s = inputs[0].to(device)\n",
    "        img3_s = inputs[1].to(device)\n",
    "        \n",
    "        images = (img1_t, img2_s, img3_s)\n",
    "        \n",
    "        label1_t = labels[0].to(device)\n",
    "        label2_s = labels[0].to(device)\n",
    "        label3_s = labels[1].to(device)\n",
    "        \n",
    "        label = (label1_t, label2_s, label3_s)\n",
    "        \n",
    "        losses, corrects = fit.test(images, label, KD_LAMBDA)\n",
    "        \n",
    "        sc.calc_sum(losses, corrects)\n",
    "    \n",
    "    if sc.score_print(len(testset), train = False):\n",
    "      sc.score_append(len(testset), train = False)\n",
    "      sc.score_del()\n",
    "      break\n",
    "\n",
    "    sc.score_append(len(testset), train = False)\n",
    "    sc.score_del()\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "v8AwJffQkGIA"
   },
   "outputs": [],
   "source": [
    "# get the scores\n",
    "train_losses, train_corrects = sc.get_value()\n",
    "test_losses, test_corrects = sc.get_value(train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hqL5rjHIkXX2"
   },
   "outputs": [],
   "source": [
    "def plot_score(epoch, train_data, test_data, x_lim = None, y_lim = None, x_label = 'EPOCH', y_label = 'score', title = 'score', legend = ['train', 'test'], filename = 'test'):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    if x_lim is None:\n",
    "        x_lim = epoch\n",
    "    if y_lim is None:\n",
    "        y_lim = 1\n",
    "        \n",
    "    plt.plot(range(epoch), train_data)\n",
    "    plt.plot(range(epoch), test_data, c='#00ff00')\n",
    "    plt.xlim(0, x_lim)\n",
    "    plt.ylim(0, y_lim)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(legend)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename+'.png')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def save_data(train_loss, test_loss, train_acc, test_acc, filename):\n",
    "    with open(filename + '.txt', mode='w') as f:\n",
    "        f.write(\"train mean loss={}\\n\".format(train_loss[-1]))\n",
    "        f.write(\"test  mean loss={}\\n\".format(test_loss[-1]))\n",
    "        f.write(\"train accuracy={}\\n\".format(train_acc[-1]))\n",
    "        f.write(\"test  accuracy={}\\n\".format(test_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_s.state_dict(), fnnname + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(train_losses[0], test_losses[0], train_corrects[0], test_corrects[0], result_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ojZoGJRRkI9S"
   },
   "outputs": [],
   "source": [
    "# output the glaphs of the scores\n",
    "\n",
    "plot_score(15, train_losses[0], test_losses[0], y_lim = 5.0, y_label = 'LOSS', legend = ['train loss', 'test loss'], title = 'total loss', filename = total_loss_name)\n",
    "\n",
    "plot_score(15, train_losses[1], test_losses[1], y_lim = 5.0, y_label = 'LOSS', legend = ['train loss', 'test loss'], title = 'softmax loss', filename = soft_loss_name)\n",
    "\n",
    "plot_score(15, train_losses[2], test_losses[2], y_lim = 5.0, y_label = 'LOSS', legend = ['train loss', 'test loss'], title = 'triplet loss', filename = tri_loss_name)\n",
    "\n",
    "plot_score(15, train_corrects[0], test_corrects[0], y_lim = 1, y_label = 'ACCURACY', legend = ['train acc', 'test acc'], title = 'accuracy', filename = acc_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TripletExperiment-Cifar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1684bb7895874598b0cb3c8344ce4851": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_185cff25aff34094a39c57dbf63e1975",
       "IPY_MODEL_cd02fad8000c476ebea64496837d52c3",
       "IPY_MODEL_4e65525bb7fa4638964b72c3490847fb"
      ],
      "layout": "IPY_MODEL_cd38ab54d77d4b0eb9f027e96800096a"
     }
    },
    "185cff25aff34094a39c57dbf63e1975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f77ef129c307426b9aaf83dd61dcc5d5",
      "placeholder": "​",
      "style": "IPY_MODEL_7a07a97d997744fab665c96c1f5fe579",
      "value": ""
     }
    },
    "4e65525bb7fa4638964b72c3490847fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85f298067f814dd19f731aebea5d6e96",
      "placeholder": "​",
      "style": "IPY_MODEL_b372328dca9b49d4960f05df1a50d60a",
      "value": " 170499072/? [00:02&lt;00:00, 56484357.94it/s]"
     }
    },
    "6c49593980e64b7fac5d3ce14efb325c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a07a97d997744fab665c96c1f5fe579": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85f298067f814dd19f731aebea5d6e96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c23bae705e54a6084b3fd4282a4ecdc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b372328dca9b49d4960f05df1a50d60a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd02fad8000c476ebea64496837d52c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c23bae705e54a6084b3fd4282a4ecdc",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c49593980e64b7fac5d3ce14efb325c",
      "value": 170498071
     }
    },
    "cd38ab54d77d4b0eb9f027e96800096a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f77ef129c307426b9aaf83dd61dcc5d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
