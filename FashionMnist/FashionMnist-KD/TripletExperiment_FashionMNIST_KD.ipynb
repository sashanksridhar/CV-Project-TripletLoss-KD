{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ct6xY1DP3VNt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X9FbV_O94JEz"
   },
   "outputs": [],
   "source": [
    "# initialize for  each parameters\n",
    "DATASET = 'FashionMNIST'\n",
    "BATCH_SIZE = 100\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "WEIGHT_DECAY = 0.007\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "SCHEDULER_STEPS = 100\n",
    "SCHEDULER_GAMMA = 0.1\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "EPOCH = 150\n",
    "\n",
    "KD_LAMBDA = 2.0\n",
    "\n",
    "TRIPLET_MARGINE = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd_scratch/cvit/sashank.sridhar\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "veXjheeA4LhC"
   },
   "outputs": [],
   "source": [
    "# fixing the seed\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnVR6kht4NZh",
    "outputId": "1a5b33ee-df16-440e-d107-ee70483ce6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu mode\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"gpu mode\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"cpu mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nS54WHBh4Pdg"
   },
   "outputs": [],
   "source": [
    "# the name of results files\n",
    "codename = 'fashion_mnist_kd'\n",
    "\n",
    "fnnname = codename + \"_fnn_model\"\n",
    "\n",
    "total_loss_name = codename + \"_total_loss\"\n",
    "soft_loss_name = codename + \"_soft_loss\"\n",
    "tri_loss_name = codename + \"_tri_loss\"\n",
    "acc_name = codename + \"_accuracy\"\n",
    "\n",
    "result_name = codename + \"_result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-uCnZzVc4S-k"
   },
   "outputs": [],
   "source": [
    "class Datasets(object):\n",
    "    def __init__(self, dataset_name, batch_size = 100, num_workers = 2, transform = None, shuffle = True):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def create(self, path = None):\n",
    "        print(\"Dataset :\",self.dataset_name)\n",
    "        if self.transform is None:\n",
    "                self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "        \n",
    "        if path is None:\n",
    "            path = \"./\"+self.dataset_name+\"Dataset/data\"\n",
    "        \n",
    "        \n",
    "        if self.dataset_name == \"MNIST\":\n",
    "            trainset = torchvision.datasets.MNIST(root = path,\n",
    "                                       train = True, download = True, transform = self.transform)\n",
    "            testset = torchvision.datasets.MNIST(root = path,\n",
    "                                                 train = False, download = True, transform = self.transform)\n",
    "            classes = list(range(10))\n",
    "            base_labels = trainset.classes\n",
    "            \n",
    "        elif self.dataset_name == \"FashionMNIST\":\n",
    "            trainset = torchvision.datasets.FashionMNIST(root = path,\n",
    "                                       train = True, download = True, transform = self.transform)\n",
    "            testset = torchvision.datasets.FashionMNIST(root = path,\n",
    "                                                 train = False, download = True, transform = self.transform)\n",
    "            classes = list(range(10))\n",
    "            base_labels = trainset.classes\n",
    "            \n",
    "        elif self.dataset_name == \"CIFAR10\":\n",
    "            trainset = torchvision.datasets.CIFAR10(root = path,\n",
    "                                       train = True, download = True, transform = self.transform)\n",
    "            testset = torchvision.datasets.CIFAR10(root = path,\n",
    "                                                 train = False, download = True, transform = self.transform)\n",
    "            classes = list(range(10))\n",
    "            base_labels = trainset.classes\n",
    "            \n",
    "        elif self.dataset_name == \"CIFAR100\":\n",
    "            trainset = torchvision.datasets.CIFAR100(root = path,\n",
    "                                       train = True, download = True, transform = self.transform)\n",
    "            testset = torchvision.datasets.CIFAR100(root = path,\n",
    "                                                 train = False, download = True, transform = self.transform)\n",
    "            classes = list(range(100))\n",
    "            base_labels = trainset.classes\n",
    "        \n",
    "        else:\n",
    "            raise KeyError(\"Unknown dataset: {}\".format(self.dataset_name))\n",
    "            \n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size = self.batch_size,\n",
    "                        shuffle = self.shuffle, num_workers = self.num_workers)\n",
    "        \n",
    "        if testset is not None:\n",
    "            testloader = torch.utils.data.DataLoader(testset, batch_size = self.batch_size,\n",
    "                        shuffle = False, num_workers = self.num_workers)\n",
    "        else:\n",
    "            testloader = None\n",
    "            \n",
    "            \n",
    "        return [trainloader, testloader, classes, base_labels, trainset, testset]\n",
    "    \n",
    "    def worker_init_fn(self, worker_id):                                                          \n",
    "        np.random.seed(worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "1684bb7895874598b0cb3c8344ce4851",
      "185cff25aff34094a39c57dbf63e1975",
      "cd02fad8000c476ebea64496837d52c3",
      "4e65525bb7fa4638964b72c3490847fb",
      "cd38ab54d77d4b0eb9f027e96800096a",
      "f77ef129c307426b9aaf83dd61dcc5d5",
      "7a07a97d997744fab665c96c1f5fe579",
      "9c23bae705e54a6084b3fd4282a4ecdc",
      "6c49593980e64b7fac5d3ce14efb325c",
      "85f298067f814dd19f731aebea5d6e96",
      "b372328dca9b49d4960f05df1a50d60a"
     ]
    },
    "id": "qLqmqtjc4ZzB",
    "outputId": "fab5611e-6dc2-4f95-b78f-cbb348fd6daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : FashionMNIST\n"
     ]
    }
   ],
   "source": [
    "# load the data set\n",
    "instance_datasets = Datasets(DATASET, BATCH_SIZE, NUM_WORKERS, shuffle = False)\n",
    "data_sets = instance_datasets.create()\n",
    "\n",
    "#trainloader = data_sets[0]\n",
    "#testloader = data_sets[1]\n",
    "classes = data_sets[2]\n",
    "based_labels = data_sets[3]\n",
    "trainset = data_sets[4]\n",
    "testset = data_sets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "E6IHN1HX4e-K"
   },
   "outputs": [],
   "source": [
    "class KDTripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        data = dataset.data\n",
    "        labels = dataset.targets\n",
    "        if type(labels) is not torch.Tensor:\n",
    "                labels = torch.tensor(labels)\n",
    "        \n",
    "        # make label set 0-9\n",
    "        labels_set = set(labels.numpy())\n",
    "        \n",
    "        # make the indices excepted each classes\n",
    "        label_to_indices = {label : np.where(labels.numpy() != label)[0] for label in labels_set}\n",
    "        \n",
    "        if self.dataset.train:\n",
    "            self.negative_indices = label_to_indices\n",
    "        else:\n",
    "            self.negative_indices = [[np.random.choice(label_to_indices[labels[i].item()])] for i in range(len(data))]\n",
    "        \n",
    "\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        if self.dataset.train:\n",
    "            img1_2, label1_2 = self.dataset[index]\n",
    "            if type(label1_2) is not torch.Tensor:\n",
    "                label1_2 = torch.tensor(label1_2)\n",
    "            img3, label3 = self.dataset[np.random.choice(self.negative_indices[label1_2.item()])]\n",
    "        else:\n",
    "            img1_2, label1_2 = self.dataset[index]\n",
    "            img3, label3 = self.dataset[self.negative_indices[index][0]]\n",
    "        \n",
    "            \n",
    "        return (img1_2, img3), (label1_2, label3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9GMBjWBp4lWX"
   },
   "outputs": [],
   "source": [
    "# use the KD Triplet Dataset by using above dataset\n",
    "tri_trainset = KDTripletDataset(trainset)\n",
    "tri_testset = KDTripletDataset(testset)\n",
    "tri_trainloader = torch.utils.data.DataLoader(tri_trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
    "tri_testloader = torch.utils.data.DataLoader(tri_testset, batch_size = BATCH_SIZE, shuffle = False, num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eN-5D93D4t6r"
   },
   "outputs": [],
   "source": [
    "class TeacherNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "        self.dropout_input = 0.0\n",
    "        self.dropout_hidden = 0.0\n",
    "        self.is_training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.dropout(x, p=self.dropout_input, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class StudentNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 400)\n",
    "        self.fc2 = nn.Linear(400, 10)\n",
    "        self.dropout_input = 0.0\n",
    "        self.dropout_hidden = 0.0\n",
    "        self.is_training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.dropout(x, p=self.dropout_input, training=self.is_training)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=self.dropout_hidden, training=self.is_training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bat_size = 100 \n",
      "loss = CrossEntropyLoss()\n",
      "optimizer = SGD\n",
      "max_epochs =  20 \n",
      "lrn_rate = 0.005 \n",
      "\n",
      "Starting training\n",
      "Iteration: 600/600train mean loss=186.42565689484277, accuracy=0.5251666666666666\n",
      "Iteration: 600/600train mean loss=103.33998413880666, accuracy=0.6680333333333335\n",
      "Iteration: 600/600train mean loss=79.87165928880374, accuracy=0.7149666666666669\n",
      "Iteration: 600/600train mean loss=70.43001314004262, accuracy=0.7532333333333338\n",
      "Iteration: 600/600train mean loss=64.35312906901042, accuracy=0.7792999999999991\n",
      "Iteration: 600/600train mean loss=59.93503623704115, accuracy=0.7968499999999995\n",
      "Iteration: 600/600train mean loss=56.65194594860077, accuracy=0.8081666666666668\n",
      "Iteration: 600/600train mean loss=54.1580665409565, accuracy=0.8154999999999993\n",
      "Iteration: 600/600train mean loss=52.21072558065256, accuracy=0.8220999999999997\n",
      "Iteration: 600/600train mean loss=50.64885779718558, accuracy=0.8266333333333334\n",
      "Iteration: 600/600train mean loss=49.364249140024185, accuracy=0.8301833333333329\n",
      "Iteration: 600/600train mean loss=48.28256228069464, accuracy=0.8338999999999994\n",
      "Iteration: 600/600train mean loss=47.35570173958937, accuracy=0.8369333333333331\n",
      "Iteration: 600/600train mean loss=46.54588244358698, accuracy=0.8393499999999999\n",
      "Iteration: 600/600train mean loss=45.82914353410403, accuracy=0.8411666666666665\n",
      "Iteration: 600/600train mean loss=45.18683850765228, accuracy=0.843933333333333\n",
      "Iteration: 600/600train mean loss=44.602609902620316, accuracy=0.8456833333333326\n",
      "Iteration: 600/600train mean loss=44.06674322485924, accuracy=0.8474333333333324\n",
      "Iteration: 600/600train mean loss=43.5728502869606, accuracy=0.8486666666666655\n",
      "Iteration: 600/600train mean loss=43.11204958458742, accuracy=0.8501666666666655\n",
      "Done \n"
     ]
    }
   ],
   "source": [
    "net = TeacherNetwork()\n",
    "bat_size = 100\n",
    "max_epochs = 20  # 100 gives better results\n",
    "ep_log_interval = 5\n",
    "lrn_rate = 0.005\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()  # does log-softmax()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lrn_rate)\n",
    "\n",
    "print(\"\\nbat_size = %3d \" % bat_size)\n",
    "print(\"loss = \" + str(loss_func))\n",
    "print(\"optimizer = SGD\")\n",
    "print(\"max_epochs = %3d \" % max_epochs)\n",
    "print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
    "\n",
    "print(\"\\nStarting training\")\n",
    "net.train()  # set mode\n",
    "\n",
    "for epoch in range(0, max_epochs):\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    ep_loss = 0  # for one full epoch\n",
    "    c = 0\n",
    "    for (batch_idx, batch) in enumerate(data_sets[0]):\n",
    "        c+=1\n",
    "        print(\"\\rIteration: {}/{}\".format(c, len(data_sets[0])), end=\"\")\n",
    "        (X, y) = batch  # X = pixels, y = target labels\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        X.to(device)\n",
    "#         print(X.shape)\n",
    "        y.to(device)\n",
    "        oupt = net(X)\n",
    "        loss_val = loss_func(oupt, y)  # a tensor\n",
    "        ep_loss += loss_val.item()  # accumulate\n",
    "        loss_val.backward()  # compute grads\n",
    "        optimizer.step()     # update weights\n",
    "        accuracy += float(torch.sum(torch.argmax(oupt, dim=1) == y).item()) / y.shape[0]\n",
    "    print(\"train mean loss={}, accuracy={}\".format(ep_loss*bat_size/len(data_sets[0]), accuracy/len(data_sets[0])))\n",
    "print(\"Done \") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"FashionMnist_teacher.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2Vay8DfU4y3z"
   },
   "outputs": [],
   "source": [
    "# network and criterions\n",
    "model_t = TeacherNetwork().to(device)\n",
    "model_s = StudentNetwork().to(device)\n",
    "\n",
    "model_t.load_state_dict(torch.load(\"FashionMnist_teacher.pt\"))\n",
    "\n",
    "optimizer = optim.SGD(model_s.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEPS, gamma=SCHEDULER_GAMMA)\n",
    "\n",
    "soft_criterion = nn.CrossEntropyLoss()\n",
    "triplet_loss = nn.TripletMarginLoss(margin=TRIPLET_MARGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "18uppIPYXFG2"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MZpk2uTX44GD"
   },
   "outputs": [],
   "source": [
    "class NetworkFit(object):\n",
    "    def __init__(self, model_t, model_s, optimizer, soft_criterion, triplet_loss):\n",
    "        self.model_t = model_t\n",
    "        self.model_s = model_s\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.soft_criterion = soft_criterion\n",
    "        self.triplet_loss = triplet_loss\n",
    "        \n",
    "        self.model_t.eval()\n",
    "        \n",
    "\n",
    "    def train(self, inputs, labels, kd_lambda = 2.0):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model_s.train()\n",
    "\n",
    "        img1_t = inputs[0]\n",
    "        img2_s = inputs[1]\n",
    "        img3_s = inputs[2]\n",
    "        \n",
    "        label1_t = labels[0]\n",
    "        label2_s = labels[1]\n",
    "        label3_s = labels[2]\n",
    "        \n",
    "        out1_t = self.model_t(img1_t)\n",
    "        out2_s = self.model_s(img2_s)\n",
    "        out3_s = self.model_s(img3_s)\n",
    "        \n",
    "        soft_loss = self.soft_criterion(out2_s, label2_s)\n",
    "        trip_loss = self.triplet_loss(out1_t, out2_s, out3_s)\n",
    "\n",
    "        loss = soft_loss + kd_lambda*trip_loss\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "            \n",
    "    def test(self, inputs, labels, kd_lambda = 2.0):\n",
    "        self.model_s.eval()\n",
    "        \n",
    "        img1_t = inputs[0]\n",
    "        img2_s = inputs[1]\n",
    "        img3_s = inputs[2]\n",
    "        \n",
    "        label1_t = labels[0]\n",
    "        label2_s = labels[1]\n",
    "        label3_s = labels[2]\n",
    "        \n",
    "        out1_t = self.model_t(img1_t)\n",
    "        out2_s = self.model_s(img2_s)\n",
    "        out3_s = self.model_s(img3_s)\n",
    "        \n",
    "        soft_loss = self.soft_criterion(out2_s, label2_s)\n",
    "        trip_loss = self.triplet_loss(out1_t, out2_s, out3_s)\n",
    "\n",
    "        loss = soft_loss + kd_lambda*trip_loss\n",
    "        \n",
    "        _, predicted = out2_s.max(1)\n",
    "        correct = (predicted == label2_s).sum().item()\n",
    "        \n",
    "        return [loss.item(), soft_loss.item(), trip_loss.item()], [correct]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "AHRH5T6J5VP-"
   },
   "outputs": [],
   "source": [
    "# fit for training and test\n",
    "fit = NetworkFit(model_t, model_s, optimizer, soft_criterion, triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "j9BF3qbE5W--"
   },
   "outputs": [],
   "source": [
    "class Score(object):\n",
    "    def __init__(self, score = 0):\n",
    "        self.score = score\n",
    "        \n",
    "    def sum_score(self, score):\n",
    "        self.score += score\n",
    "    \n",
    "    def set_score(self, score):\n",
    "        self.score = score\n",
    "    \n",
    "    def init_score(self):\n",
    "        self.score = 0\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U0gzfZ-n5hA0"
   },
   "outputs": [],
   "source": [
    "class ScoreCalc(object):\n",
    "    def __init__(self, losses, corrects, batch_size):\n",
    "        self.losses = losses\n",
    "        self.corrects = corrects\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.len_l = len(losses)\n",
    "        self.len_c = len(corrects)\n",
    "        \n",
    "        self.train_losses = [[] for l in range(self.len_l)]\n",
    "        self.train_corrects = [[] for c in range(self.len_c)]\n",
    "        \n",
    "        self.test_losses = [[] for l in range(self.len_l)]\n",
    "        self.test_corrects = [[] for c in range(self.len_c)]\n",
    "\n",
    "        patience = 20\n",
    "\n",
    "        self.early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "       \n",
    "    \n",
    "    def calc_sum(self, losses, corrects):\n",
    "        if len(losses) != len(self.losses):\n",
    "            print(\"warning : len(losses) != len(self.losses)\")\n",
    "            sys.exit()\n",
    "        if len(corrects) != len(self.corrects):\n",
    "            print(\"warning : len(corrects) != len(self.corrects)\")\n",
    "            sys.exit()\n",
    "        \n",
    "        for l in range(self.len_l):\n",
    "            self.losses[l].sum_score(losses[l])\n",
    "        \n",
    "        for c in range(self.len_c):\n",
    "            self.corrects[c].sum_score(corrects[c])\n",
    "        \n",
    "        return self.losses, self.corrects\n",
    "    \n",
    "    \n",
    "    def score_del(self):\n",
    "        for loss in self.losses:\n",
    "            loss.init_score()\n",
    "        for correct in self.corrects:\n",
    "            correct.init_score()\n",
    "\n",
    "        \n",
    "    def score_print(self, data_num, train = True):\n",
    "        if train:\n",
    "            print(\"train mean loss={}, accuracy={}\".format(self.losses[0].get_score()*self.batch_size/data_num, float(self.corrects[0].get_score()/data_num)))\n",
    "        else:\n",
    "            \n",
    "            print(\"test mean loss={}, accuracy={}\".format(self.losses[0].get_score()*self.batch_size/data_num, float(self.corrects[0].get_score()/data_num)))\n",
    "            self.early_stopping(self.losses[0].get_score()*self.batch_size/data_num, model_s)\n",
    "        \n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                return True\n",
    "            else:\n",
    "              return False\n",
    "\n",
    "            \n",
    "    def score_append(self, data_num, train = True):\n",
    "        if train:\n",
    "            for l in range(self.len_l):\n",
    "                self.train_losses[l].append(self.losses[l].get_score()*self.batch_size/data_num)\n",
    "            for c in range(self.len_c):\n",
    "                self.train_corrects[c].append(float(self.corrects[c].get_score()/data_num))\n",
    "        else:\n",
    "            for l in range(self.len_l):\n",
    "                self.test_losses[l].append(self.losses[l].get_score()*self.batch_size/data_num)\n",
    "            for c in range(self.len_c):\n",
    "                self.test_corrects[c].append(float(self.corrects[c].get_score()/data_num))\n",
    "    \n",
    "    \n",
    "    def get_value(self, train = True):\n",
    "        if train:\n",
    "            return self.train_losses, self.train_corrects\n",
    "        else:\n",
    "            return self.test_losses, self.test_corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "W4ogtBkH5cHu"
   },
   "outputs": [],
   "source": [
    "# to manage all scores\n",
    "loss = Score()\n",
    "loss_s = Score()\n",
    "loss_t = Score()\n",
    "correct = Score()\n",
    "score_loss = [loss, loss_s, loss_t]\n",
    "score_correct = [correct]\n",
    "sc = ScoreCalc(score_loss, score_correct, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxeumM7g5czi",
    "outputId": "ce7864d0-f67e-4fe8-dd88-9b9aa803ab43",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.49753295523424945, accuracy=0.8422166666666666\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5296278563141823, accuracy=0.8295\n",
      "Validation loss decreased (inf --> 0.529628).  Saving model ...\n",
      "epoch 2\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5216366605460644, accuracy=0.83915\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5528769880533219, accuracy=0.8269\n",
      "EarlyStopping counter: 1 out of 20\n",
      "epoch 3\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5421613029638926, accuracy=0.8330833333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5746130689978599, accuracy=0.8186\n",
      "EarlyStopping counter: 2 out of 20\n",
      "epoch 4\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.511372955540816, accuracy=0.8466166666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5458123055100441, accuracy=0.833\n",
      "EarlyStopping counter: 3 out of 20\n",
      "epoch 5\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.48067819366852443, accuracy=0.85075\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5140216460824013, accuracy=0.8349\n",
      "Validation loss decreased (0.529628 --> 0.514022).  Saving model ...\n",
      "epoch 6\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.49696392675240836, accuracy=0.8470333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5270493122935295, accuracy=0.8345\n",
      "EarlyStopping counter: 1 out of 20\n",
      "epoch 7\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4678944902122021, accuracy=0.85545\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.49959503263235094, accuracy=0.8427\n",
      "Validation loss decreased (0.514022 --> 0.499595).  Saving model ...\n",
      "epoch 8\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.48520664078493914, accuracy=0.8513333333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5183010101318359, accuracy=0.8379\n",
      "EarlyStopping counter: 1 out of 20\n",
      "epoch 9\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5534090250730515, accuracy=0.8321833333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5917717123031616, accuracy=0.8219\n",
      "EarlyStopping counter: 2 out of 20\n",
      "epoch 10\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4738568144539992, accuracy=0.8542833333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.50958985298872, accuracy=0.839\n",
      "EarlyStopping counter: 3 out of 20\n",
      "epoch 11\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5719038107991219, accuracy=0.8442\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.6036042520403861, accuracy=0.8273\n",
      "EarlyStopping counter: 4 out of 20\n",
      "epoch 12\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4793172948559125, accuracy=0.8520666666666666\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5151261055469513, accuracy=0.8375\n",
      "EarlyStopping counter: 5 out of 20\n",
      "epoch 13\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.49020769288142524, accuracy=0.85685\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5234802854061127, accuracy=0.8448\n",
      "EarlyStopping counter: 6 out of 20\n",
      "epoch 14\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.45429475588103135, accuracy=0.8580166666666666\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.4930882832407951, accuracy=0.8417\n",
      "Validation loss decreased (0.499595 --> 0.493088).  Saving model ...\n",
      "epoch 15\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4865924589087566, accuracy=0.8501666666666666\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5226751574873925, accuracy=0.8372\n",
      "EarlyStopping counter: 1 out of 20\n",
      "epoch 16\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5173690318564574, accuracy=0.8391833333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5601350861787796, accuracy=0.8247\n",
      "EarlyStopping counter: 2 out of 20\n",
      "epoch 17\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4852898525943359, accuracy=0.85225\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5247872567176819, accuracy=0.8376\n",
      "EarlyStopping counter: 3 out of 20\n",
      "epoch 18\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5025989539921284, accuracy=0.845\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5338670471310616, accuracy=0.8305\n",
      "EarlyStopping counter: 4 out of 20\n",
      "epoch 19\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4776247016588847, accuracy=0.8529833333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5166280770301819, accuracy=0.8381\n",
      "EarlyStopping counter: 5 out of 20\n",
      "epoch 20\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.48015994956096014, accuracy=0.85615\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5167732480168342, accuracy=0.8411\n",
      "EarlyStopping counter: 6 out of 20\n",
      "epoch 21\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.45955433207253615, accuracy=0.8586333333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.4896096447110176, accuracy=0.8457\n",
      "Validation loss decreased (0.493088 --> 0.489610).  Saving model ...\n",
      "epoch 22\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4816298897564411, accuracy=0.85135\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5191117203235627, accuracy=0.8341\n",
      "EarlyStopping counter: 1 out of 20\n",
      "epoch 23\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4569690746317307, accuracy=0.8595\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.49089267283678056, accuracy=0.8439\n",
      "EarlyStopping counter: 2 out of 20\n",
      "epoch 24\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.48857846620182194, accuracy=0.8424666666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5319457739591599, accuracy=0.8261\n",
      "EarlyStopping counter: 3 out of 20\n",
      "epoch 25\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5197614654650291, accuracy=0.8442\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5555833619832993, accuracy=0.8308\n",
      "EarlyStopping counter: 4 out of 20\n",
      "epoch 26\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.46699976896246276, accuracy=0.85375\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5027476134896278, accuracy=0.8372\n",
      "EarlyStopping counter: 5 out of 20\n",
      "epoch 27\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4770679990450541, accuracy=0.8522166666666666\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5147077569365501, accuracy=0.839\n",
      "EarlyStopping counter: 6 out of 20\n",
      "epoch 28\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.49347961296637854, accuracy=0.8552833333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.530378351509571, accuracy=0.8409\n",
      "EarlyStopping counter: 7 out of 20\n",
      "epoch 29\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4817893230418364, accuracy=0.8488666666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.516636055111885, accuracy=0.8315\n",
      "EarlyStopping counter: 8 out of 20\n",
      "epoch 30\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5324034356077512, accuracy=0.8542333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5713449075818062, accuracy=0.8418\n",
      "EarlyStopping counter: 9 out of 20\n",
      "epoch 31\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.47702572199205556, accuracy=0.8566833333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5136944508552551, accuracy=0.8396\n",
      "EarlyStopping counter: 10 out of 20\n",
      "epoch 32\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.49403941941757995, accuracy=0.8552166666666666\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5273791953921318, accuracy=0.8458\n",
      "EarlyStopping counter: 11 out of 20\n",
      "epoch 33\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5079206988463799, accuracy=0.843\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5471952429413796, accuracy=0.8255\n",
      "EarlyStopping counter: 12 out of 20\n",
      "epoch 34\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5139821333189806, accuracy=0.8544166666666667\n",
      "Test Loss Calc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100/100test mean loss=0.5454268109798431, accuracy=0.8393\n",
      "EarlyStopping counter: 13 out of 20\n",
      "epoch 35\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.45187087267637255, accuracy=0.8592666666666666\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.4862870985269547, accuracy=0.8455\n",
      "Validation loss decreased (0.489610 --> 0.486287).  Saving model ...\n",
      "epoch 36\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4775465101003647, accuracy=0.8535833333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5081439736485481, accuracy=0.8392\n",
      "EarlyStopping counter: 1 out of 20\n",
      "epoch 37\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.47400624374548594, accuracy=0.8529333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5052157285809517, accuracy=0.8395\n",
      "EarlyStopping counter: 2 out of 20\n",
      "epoch 38\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.494656131764253, accuracy=0.8505666666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5250608906149864, accuracy=0.8372\n",
      "EarlyStopping counter: 3 out of 20\n",
      "epoch 39\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.46335627677539987, accuracy=0.8561333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.49674812465906143, accuracy=0.8426\n",
      "EarlyStopping counter: 4 out of 20\n",
      "epoch 40\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5056282417476177, accuracy=0.8402333333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5432334583997727, accuracy=0.8256\n",
      "EarlyStopping counter: 5 out of 20\n",
      "epoch 41\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.5092596298952897, accuracy=0.8501166666666666\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5439108893275261, accuracy=0.8352\n",
      "EarlyStopping counter: 6 out of 20\n",
      "epoch 42\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4590768043200175, accuracy=0.8573666666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.4961026307940483, accuracy=0.8436\n",
      "EarlyStopping counter: 7 out of 20\n",
      "epoch 43\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.471892247547706, accuracy=0.8508666666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5081593248248101, accuracy=0.835\n",
      "EarlyStopping counter: 8 out of 20\n",
      "epoch 44\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.47636319359143575, accuracy=0.85115\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5163082695007324, accuracy=0.8329\n",
      "EarlyStopping counter: 9 out of 20\n",
      "epoch 45\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4582147011657556, accuracy=0.86175\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.4927498042583466, accuracy=0.8454\n",
      "EarlyStopping counter: 10 out of 20\n",
      "epoch 46\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4760398241629203, accuracy=0.8555\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5131360614299774, accuracy=0.8425\n",
      "EarlyStopping counter: 11 out of 20\n",
      "epoch 47\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.49788198843598364, accuracy=0.8437666666666667\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5332417216897011, accuracy=0.8324\n",
      "EarlyStopping counter: 12 out of 20\n",
      "epoch 48\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.45086630719403425, accuracy=0.8603833333333334\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.48914126604795455, accuracy=0.8438\n",
      "EarlyStopping counter: 13 out of 20\n",
      "epoch 49\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4596310014029344, accuracy=0.8568333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.49625363558530805, accuracy=0.8417\n",
      "EarlyStopping counter: 14 out of 20\n",
      "epoch 50\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4765556822468837, accuracy=0.8577333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5104868173599243, accuracy=0.8387\n",
      "EarlyStopping counter: 15 out of 20\n",
      "epoch 51\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.46867871935168903, accuracy=0.8564\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5001048198342324, accuracy=0.8415\n",
      "EarlyStopping counter: 16 out of 20\n",
      "epoch 52\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.489490101536115, accuracy=0.8501333333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5198681351542472, accuracy=0.8375\n",
      "EarlyStopping counter: 17 out of 20\n",
      "epoch 53\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.49190913985172907, accuracy=0.85085\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5313320764899254, accuracy=0.8371\n",
      "EarlyStopping counter: 18 out of 20\n",
      "epoch 54\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4788656555612882, accuracy=0.8532833333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5127237367630005, accuracy=0.8403\n",
      "EarlyStopping counter: 19 out of 20\n",
      "epoch 55\n",
      "Iteration: 600/600Train Loss Calc\n",
      "Iteration: 600/600train mean loss=0.4790559227267901, accuracy=0.8547833333333333\n",
      "Test Loss Calc\n",
      "Iteration: 100/100test mean loss=0.5199884808063507, accuracy=0.8396\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# training and test\n",
    "for epoch in range(EPOCH):\n",
    "    print('epoch', epoch+1)\n",
    "    \n",
    "    c = 0\n",
    "    for (inputs, labels) in tri_trainloader:\n",
    "        c+=1\n",
    "        print(\"\\rIteration: {}/{}\".format(c, len(tri_trainloader)), end=\"\")\n",
    "        img1_t = inputs[0].to(device)\n",
    "        img2_s = inputs[0].to(device)\n",
    "        img3_s = inputs[1].to(device)\n",
    "        \n",
    "        images = (img1_t, img2_s, img3_s)\n",
    "        \n",
    "        label1_t = labels[0].to(device)\n",
    "        label2_s = labels[0].to(device)\n",
    "        label3_s = labels[1].to(device)\n",
    "        \n",
    "        label = (label1_t, label2_s, label3_s)\n",
    "        \n",
    "        fit.train(images, label, KD_LAMBDA)\n",
    "    c = 0\n",
    "    print(\"Train Loss Calc\")\n",
    "    for (inputs, labels) in tri_trainloader:\n",
    "        c+=1\n",
    "        print(\"\\rIteration: {}/{}\".format(c, len(tri_trainloader)), end=\"\")\n",
    "        img1_t = inputs[0].to(device)\n",
    "        img2_s = inputs[0].to(device)\n",
    "        img3_s = inputs[1].to(device)\n",
    "        \n",
    "        images = (img1_t, img2_s, img3_s)\n",
    "        \n",
    "        label1_t = labels[0].to(device)\n",
    "        label2_s = labels[0].to(device)\n",
    "        label3_s = labels[1].to(device)\n",
    "        \n",
    "        label = (label1_t, label2_s, label3_s)\n",
    "        \n",
    "        losses, corrects = fit.test(images, label, KD_LAMBDA)\n",
    "        \n",
    "        sc.calc_sum(losses, corrects)\n",
    "    \n",
    "    sc.score_print(len(trainset))\n",
    "    sc.score_append(len(trainset))\n",
    "    sc.score_del()\n",
    "    c = 0\n",
    "    print(\"Test Loss Calc\")\n",
    "    for (inputs, labels) in tri_testloader:\n",
    "        c+=1\n",
    "        \n",
    "        print(\"\\rIteration: {}/{}\".format(c, len(tri_testloader)), end=\"\")\n",
    "        img1_t = inputs[0].to(device)\n",
    "        img2_s = inputs[0].to(device)\n",
    "        img3_s = inputs[1].to(device)\n",
    "        \n",
    "        images = (img1_t, img2_s, img3_s)\n",
    "        \n",
    "        label1_t = labels[0].to(device)\n",
    "        label2_s = labels[0].to(device)\n",
    "        label3_s = labels[1].to(device)\n",
    "        \n",
    "        label = (label1_t, label2_s, label3_s)\n",
    "        \n",
    "        losses, corrects = fit.test(images, label, KD_LAMBDA)\n",
    "        \n",
    "        sc.calc_sum(losses, corrects)\n",
    "    \n",
    "    if sc.score_print(len(testset), train = False):\n",
    "      sc.score_append(len(testset), train = False)\n",
    "      sc.score_del()\n",
    "      break\n",
    "\n",
    "    sc.score_append(len(testset), train = False)\n",
    "    sc.score_del()\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "v8AwJffQkGIA"
   },
   "outputs": [],
   "source": [
    "# get the scores\n",
    "train_losses, train_corrects = sc.get_value()\n",
    "test_losses, test_corrects = sc.get_value(train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hqL5rjHIkXX2"
   },
   "outputs": [],
   "source": [
    "def plot_score(epoch, train_data, test_data, x_lim = None, y_lim = None, x_label = 'EPOCH', y_label = 'score', title = 'score', legend = ['train', 'test'], filename = 'test'):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    if x_lim is None:\n",
    "        x_lim = epoch\n",
    "    if y_lim is None:\n",
    "        y_lim = 1\n",
    "        \n",
    "    plt.plot(range(epoch), train_data)\n",
    "    plt.plot(range(epoch), test_data, c='#00ff00')\n",
    "    plt.xlim(0, x_lim)\n",
    "    plt.ylim(0, y_lim)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(legend)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename+'.png')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def save_data(train_loss, test_loss, train_acc, test_acc, filename):\n",
    "    with open(filename + '.txt', mode='w') as f:\n",
    "        f.write(\"train mean loss={}\\n\".format(train_loss[-1]))\n",
    "        f.write(\"test  mean loss={}\\n\".format(test_loss[-1]))\n",
    "        f.write(\"train accuracy={}\\n\".format(train_acc[-1]))\n",
    "        f.write(\"test  accuracy={}\\n\".format(test_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_s.state_dict(), fnnname + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(train_losses[0], test_losses[0], train_corrects[0], test_corrects[0], result_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ojZoGJRRkI9S"
   },
   "outputs": [],
   "source": [
    "# output the glaphs of the scores\n",
    "\n",
    "plot_score(150, train_losses[0], test_losses[0], y_lim = 5.0, y_label = 'LOSS', legend = ['train loss', 'test loss'], title = 'total loss', filename = total_loss_name)\n",
    "\n",
    "plot_score(150, train_losses[1], test_losses[1], y_lim = 5.0, y_label = 'LOSS', legend = ['train loss', 'test loss'], title = 'softmax loss', filename = soft_loss_name)\n",
    "\n",
    "plot_score(150, train_losses[2], test_losses[2], y_lim = 5.0, y_label = 'LOSS', legend = ['train loss', 'test loss'], title = 'triplet loss', filename = tri_loss_name)\n",
    "\n",
    "plot_score(150, train_corrects[0], test_corrects[0], y_lim = 1, y_label = 'ACCURACY', legend = ['train acc', 'test acc'], title = 'accuracy', filename = acc_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TripletExperiment-Cifar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1684bb7895874598b0cb3c8344ce4851": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_185cff25aff34094a39c57dbf63e1975",
       "IPY_MODEL_cd02fad8000c476ebea64496837d52c3",
       "IPY_MODEL_4e65525bb7fa4638964b72c3490847fb"
      ],
      "layout": "IPY_MODEL_cd38ab54d77d4b0eb9f027e96800096a"
     }
    },
    "185cff25aff34094a39c57dbf63e1975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f77ef129c307426b9aaf83dd61dcc5d5",
      "placeholder": "",
      "style": "IPY_MODEL_7a07a97d997744fab665c96c1f5fe579",
      "value": ""
     }
    },
    "4e65525bb7fa4638964b72c3490847fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85f298067f814dd19f731aebea5d6e96",
      "placeholder": "",
      "style": "IPY_MODEL_b372328dca9b49d4960f05df1a50d60a",
      "value": " 170499072/? [00:02&lt;00:00, 56484357.94it/s]"
     }
    },
    "6c49593980e64b7fac5d3ce14efb325c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a07a97d997744fab665c96c1f5fe579": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85f298067f814dd19f731aebea5d6e96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c23bae705e54a6084b3fd4282a4ecdc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b372328dca9b49d4960f05df1a50d60a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd02fad8000c476ebea64496837d52c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c23bae705e54a6084b3fd4282a4ecdc",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c49593980e64b7fac5d3ce14efb325c",
      "value": 170498071
     }
    },
    "cd38ab54d77d4b0eb9f027e96800096a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f77ef129c307426b9aaf83dd61dcc5d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
